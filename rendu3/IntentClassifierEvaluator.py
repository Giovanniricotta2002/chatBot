"""
IntentClassifierEvaluator
========================

Module d'√©valuation pour classificateur d'intentions de chatbot.

Ce module fournit la classe IntentClassifierEvaluator, qui permet d'√©valuer un mod√®le de classification d'intentions (scikit-learn compatible) sur un jeu de donn√©es fran√ßais, avec split train/test ou Leave-One-Out, et tests obligatoires.

D√©pendances :
- scikit-learn
- numpy
- collections

Exemple d'utilisation :
    >>> evaluator = IntentClassifierEvaluator(clf, texts, labels)
    >>> evaluator.evaluate()
    >>> evaluator.test_obligatoires()
"""

from collections import Counter
import numpy as np
from sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

class IntentClassifierEvaluator:
    """
    Classe utilitaire pour √©valuer un classificateur d'intentions (IntentClassifier)
    sur un jeu de donn√©es, avec gestion automatique du split train/test ou
    validation crois√©e Leave-One-Out selon la taille du dataset.
    Fournit aussi des tests obligatoires sur des phrases personnalis√©es.
    """
    def __init__(self, classifier, texts, labels):
        """
        Initialise l'√©valuateur avec un classificateur, les textes et les labels.
        
        Args:
            classifier: Instance de la classe IntentClassifier (ou pipeline compatible sklearn).
            texts (list of str): Liste des phrases d'entra√Ænement.
            labels (list of str): Liste des labels/intents correspondants.
        """
        self.classifier = classifier
        self.texts = texts
        self.labels = labels

    def evaluate(self):
        print("\n[IntentClassifierEvaluator] D√©but de l'√©valuation...")
        print(f"[IntentClassifierEvaluator] Nombre total d'exemples: {len(self.texts)}")
        print(f"[IntentClassifierEvaluator] Nombre d'intentions: {len(set(self.labels))}")
        print(f"[IntentClassifierEvaluator] D√©tail des classes: {Counter(self.labels)}")

        class_counts = Counter(self.labels)
        min_samples_per_class = min(class_counts.values())
        total_classes = len(class_counts)
        test_size = max(1, int(0.2 * len(self.texts)))

        print(f"\nüìä Donn√©es d'entra√Ænement pr√©par√©es:")
        print(f"   - Nombre d'exemples: {len(self.texts)}")
        print(f"   - Intentions disponibles: {set(self.labels)}")
        for intention in set(self.labels):
            count = self.labels.count(intention)
            print(f"     ‚Ä¢ {intention}: {count} exemples")

        if len(self.texts) == 0:
            print("‚ùå Aucune donn√©e d'entra√Ænement trouv√©e dans le dataset!")
            return

        if test_size < total_classes or min_samples_per_class < 2:
            print("\n‚ö†Ô∏è  Dataset trop petit ou test set trop petit pour un split train/test stratifi√©.")
            print(f"   - Total d'exemples: {len(self.texts)}")
            print(f"   - Nombre de classes: {total_classes}")
            print(f"   - Minimum d'exemples par classe: {min_samples_per_class}")
            print(f"   - Taille du test set calcul√©e: {test_size}")
            print(f"   - Utilisation de validation crois√©e Leave-One-Out.")
            loo = LeaveOneOut()
            print("[IntentClassifierEvaluator] Lancement de la validation crois√©e Leave-One-Out...")
            scores = cross_val_score(self.classifier.pipeline, np.array(self.texts), np.array(self.labels), cv=loo, scoring='accuracy')
            print(f"[IntentClassifierEvaluator] R√©sultats Leave-One-Out: scores={scores}")
            print(f"Pr√©cision Leave-One-Out: {scores.mean():.2f} ¬± {scores.std():.2f}")
            if scores.mean() > 0.8:
                print(f"üéØ Objectif atteint ! Pr√©cision > 80%")
            else:
                print(f"‚ùå Objectif non atteint. Pr√©cision: {scores.mean()*100:.1f}%")
        else:
            print("[IntentClassifierEvaluator] Split train/test stratifi√© 80/20...")
            X_train, X_test, y_train, y_test = train_test_split(
                self.texts, self.labels, test_size=0.2, random_state=42, stratify=self.labels)
            print(f"[IntentClassifierEvaluator] Taille train: {len(X_train)}, test: {len(X_test)}")
            self.classifier.train(X_train, y_train)
            print("[IntentClassifierEvaluator] Pr√©diction sur le test set...")
            y_pred = self.classifier.pipeline.predict(X_test)
            acc = accuracy_score(y_test, y_pred)
            prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
            print(f"\n[IntentClassifierEvaluator] R√©sultats sur le test set:")
            print(f"  - Pr√©cision: {acc:.2f}")
            print(f"  - Pr√©cision (weighted): {prec:.2f}")
            print(f"  - Rappel (weighted): {rec:.2f}")
            print(f"  - F1-score (weighted): {f1:.2f}")
            print(f"\nMatrice de confusion:")
            print(confusion_matrix(y_test, y_pred))
            print(f"\nRapport d√©taill√©:\n{classification_report(y_test, y_pred)}")
            if acc > 0.8:
                print(f"üéØ Objectif atteint ! Pr√©cision > 80%")
            else:
                print(f"‚ùå Objectif non atteint. Pr√©cision: {acc*100:.1f}%")
        print("\n[IntentClassifierEvaluator] √âvaluation termin√©e!")
        print("=" * 60)

    def test_obligatoires(self):
        print("\n[IntentClassifierEvaluator] Lancement des tests obligatoires...")
        print(f"[IntentClassifierEvaluator] Nombre d'exemples d'entra√Ænement: {len(self.texts)}")
        print(f"[IntentClassifierEvaluator] Nombre d'intentions: {len(set(self.labels))}")
        # Toujours entra√Æner sur tout le dataset avant les tests obligatoires
        self.classifier.train(self.texts, self.labels)
        print("[IntentClassifierEvaluator] Classifieur entra√Æn√© sur tout le dataset pour les tests obligatoires.")
        print("\nüß™ Tests obligatoires sur des phrases personnalis√©es :")
        test_phrases = [
            # salutation
            "Bonjour !",
            "Salut √† tous",
            "Coucou, comment √ßa va ?",
            "Hey tout le monde",
            # menu
            "Puis-je voir la carte ?",
            "Qu'est-ce qu'il y a √† manger ?",
            "Affichez-moi le menu s'il vous pla√Æt",
            # commande
            "Je veux commander trois pizzas margherita",
            "Je voudrais commander un menu pour deux personnes",
            "Commande pour emporter, s'il vous pla√Æt",
            # horaires
            "√Ä quelle heure ouvrez-vous le matin ?",
            "√ätes-vous ouverts le dimanche ?",
            "Quand fermez-vous ce soir ?",
            # prix
            "Quel est le prix d'une pizza 4 fromages ?",
            "C'est combien pour un menu complet ?",
            "Combien co√ªte une boisson ?",
            # au_revoir
            "Merci, au revoir !",
            "Bonne journ√©e √† vous",
            "√Ä la prochaine !",
            "Bye bye"
        ]
        for phrase in test_phrases:
            print(f"[IntentClassifierEvaluator] Test sur: '{phrase}'")
            prediction = self.classifier.predict(phrase)
            print(f"  ‚Üí '{phrase}'  =>  intention pr√©dite : {prediction[0]}")
        print("\n‚úÖ Tous les tests obligatoires ont √©t√© ex√©cut√©s.")
        print("=" * 60)
