import nltk
import re
import string
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

class TextPreprocessor:
    """
    Classe pour le preprocessing de texte avec NLTK.
    
    Cette classe fournit des m√©thodes pour nettoyer et pr√©parer
    les messages utilisateurs pour un syst√®me de chatbot.
    """
    
    def __init__(self, language='french', download_resources=True):
        """
        Initialise le preprocesseur de texte.
        
        Args:
            language (str): Langue pour le traitement (d√©faut: 'french')
            download_resources (bool): T√©l√©charger automatiquement les ressources NLTK
        """
        self.language = language
        self.stemmer = SnowballStemmer(language)
        self.stop_words = None
        
        if download_resources:
            self._download_nltk_resources()
            
        # Charger les stop words
        try:
            self.stop_words = set(stopwords.words(language))
        except LookupError:
            print(f"Attention: Stop words pour '{language}' non disponibles")
            self.stop_words = set()
    
    def _download_nltk_resources(self):
        """T√©l√©charge les ressources NLTK n√©cessaires."""
        resources = ['punkt', 'stopwords']
        
        for resource in resources:
            try:
                if resource == 'punkt':
                    nltk.data.find('tokenizers/punkt')
                elif resource == 'stopwords':
                    nltk.data.find('corpora/stopwords')
            except LookupError:
                print(f"T√©l√©chargement de la ressource '{resource}'...")
                nltk.download(resource)
    
    def tokenize_message(self, text):
        """
        Tokenise un message en mots et phrases.
        
        Args:
            text (str): Le texte √† tokeniser
            
        Returns:
            dict: Dictionnaire contenant les tokens de mots et phrases
        """
        words = word_tokenize(text, language=self.language)
        sentences = sent_tokenize(text, language=self.language)
        
        return {
            'words': words,
            'sentences': sentences
        }
    
    def normalize_text(self, text):
        """
        Normalise le texte selon les consignes :
        - Conversion en minuscules
        - Suppression de la ponctuation
        - Suppression des espaces multiples
        
        Args:
            text (str): Le texte √† normaliser
            
        Returns:
            str: Le texte normalis√©
        """
        # Conversion en minuscules
        text = text.lower()
        
        # Suppression de la ponctuation
        text = text.translate(str.maketrans('', '', string.punctuation))
        
        # Suppression des espaces multiples
        text = re.sub(r'\s+', ' ', text)
        
        # Suppression des espaces en d√©but et fin
        text = text.strip()
        
        return text
    
    def remove_stopwords(self, tokens):
        """
        Retire les mots vides.
        
        Args:
            tokens (list): Liste des tokens √† filtrer
            
        Returns:
            list: Liste des tokens sans les mots vides
        """
        if not self.stop_words:
            return tokens
            
        # Filtrer les tokens qui ne sont pas des mots vides
        filtered_tokens = [token for token in tokens if token.lower() not in self.stop_words]
        
        return filtered_tokens
    
    def stem_tokens(self, tokens):
        """
        R√©duit les mots √† leur racine.
        
        Args:
            tokens (list): Liste des tokens √† raciner
            
        Returns:
            list: Liste des tokens racin√©s
        """
        # Appliquer le stemming √† chaque token
        stemmed_tokens = [self.stemmer.stem(token) for token in tokens]
        
        return stemmed_tokens
    
    def preprocess_message(self, text, verbose=True):
        """
        Fonction principale qui applique tout le preprocessing.
        
        Args:
            text (str): Le message √† pr√©processer
            verbose (bool): Afficher les √©tapes de preprocessing
            
        Returns:
            dict: Dictionnaire contenant toutes les √©tapes du preprocessing
        """
        if verbose:
            print(f"\nüìù Message original: '{text}'")
        
        # 1. Tokenisation
        tokens_data = self.tokenize_message(text)
        if verbose:
            print(f"üî§ Tokens (mots): {tokens_data['words']}")
            print(f"üìÑ Tokens (phrases): {tokens_data['sentences']}")
        
        # 2. Normalisation
        normalized_text = self.normalize_text(text)
        if verbose:
            print(f"üîÑ Texte normalis√©: '{normalized_text}'")
        
        # 3. Tokenisation du texte normalis√©
        normalized_tokens = word_tokenize(normalized_text, language=self.language)
        if verbose:
            print(f"üî§ Tokens normalis√©s: {normalized_tokens}")
        
        # 4. Suppression des mots vides
        filtered_tokens = self.remove_stopwords(normalized_tokens)
        if verbose:
            print(f"üö´ Sans mots vides: {filtered_tokens}")
        
        # 5. Stemming
        stemmed_tokens = self.stem_tokens(filtered_tokens)
        if verbose:
            print(f"üå± Apr√®s stemming: {stemmed_tokens}")
        
        return {
            'original': text,
            'tokens': tokens_data,
            'normalized': normalized_text,
            'normalized_tokens': normalized_tokens,
            'filtered_tokens': filtered_tokens,
            'stemmed_tokens': stemmed_tokens
        }
    
    def process_batch(self, texts, verbose=False):
        """
        Traite une liste de textes en lot.
        
        Args:
            texts (list): Liste des textes √† traiter
            verbose (bool): Afficher les d√©tails pour chaque texte
            
        Returns:
            list: Liste des r√©sultats de preprocessing
        """
        results = []
        for text in texts:
            result = self.preprocess_message(text, verbose=verbose)
            results.append(result)
        return results
