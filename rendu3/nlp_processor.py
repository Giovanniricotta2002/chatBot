"""
nlp_processor
=============

Module d'analyse linguistique avancÃ©e pour chatbot franÃ§ais utilisant spaCy.

Ce module fournit la classe NLPProcessor, qui permet d'effectuer l'analyse morpho-syntaxique, l'extraction d'entitÃ©s nommÃ©es et l'analyse des dÃ©pendances sur des textes en franÃ§ais.

DÃ©pendances :
- spacy

Exemple d'utilisation :
    >>> nlp = NLPProcessor()
    >>> result = nlp.full_analysis("Bonjour, je m'appelle Paul.")
"""

import spacy
from typing import Dict, List, Any

class NLPProcessor:
    """
    Classe pour l'analyse linguistique avancÃ©e avec spaCy.
    
    Cette classe fournit des mÃ©thodes pour l'analyse morpho-syntaxique,
    la reconnaissance d'entitÃ©s nommÃ©es et l'analyse des dÃ©pendances syntaxiques.
    """
    
    def __init__(self, model_name="fr_core_news_sm"):
        print(f"[NLPProcessor] Chargement du modÃ¨le spaCy '{model_name}'...")
        """
        Initialise le processeur NLP.
        
        Args:
            model_name (str): Nom du modÃ¨le spaCy Ã  charger
        """
        try:
            self.nlp = spacy.load(model_name)
            print(f"âœ… ModÃ¨le spaCy '{model_name}' chargÃ© avec succÃ¨s")
        except OSError:
            print(f"âŒ Erreur: Le modÃ¨le '{model_name}' n'est pas installÃ©")
            print("ğŸ’¡ Installez-le avec: python -m spacy download fr_core_news_sm")
            raise
    
    def analyze_pos(self, text: str) -> Dict[str, Any]:
        print(f"[NLPProcessor] Analyse POS pour: '{text}'")
        """
        Analyse les parties du discours et retourne les entitÃ©s nommÃ©es.
        
        Args:
            text (str): Le texte Ã  analyser
            
        Returns:
            dict: Dictionnaire contenant l'analyse POS et les entitÃ©s
        """
        doc = self.nlp(text)
        
        # Analyse des parties du discours
        pos_analysis = []
        for token in doc:
            pos_analysis.append({
                'text': token.text,
                'lemma': token.lemma_,
                'pos': token.pos_,
                'tag': token.tag_,
                'dep': token.dep_,
                'is_alpha': token.is_alpha,
                'is_stop': token.is_stop,
                'is_punct': token.is_punct,
                'shape': token.shape_
            })
        
        # EntitÃ©s nommÃ©es
        entities = []
        for ent in doc.ents:
            entities.append({
                'text': ent.text,
                'label': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char
            })
        
        return {
            'pos_analysis': pos_analysis,
            'entities': entities,
            'doc_info': {
                'length': len(doc),
                'sentences': len(list(doc.sents))
            }
        }
    
    def extract_entities(self, text: str) -> Dict[str, List[Dict]]:
        print(f"[NLPProcessor] Extraction des entitÃ©s pour: '{text}'")
        """
        Extraire personnes, lieux, organisations, etc.
        
        Args:
            text (str): Le texte Ã  analyser
            
        Returns:
            dict: Dictionnaire des entitÃ©s par catÃ©gorie
        """
        doc = self.nlp(text)
        
        entities_by_type = {
            'PERSON': [],      # Personnes
            'ORG': [],         # Organisations
            'GPE': [],         # EntitÃ©s gÃ©opolitiques (pays, villes)
            'LOC': [],         # Lieux
            'DATE': [],        # Dates
            'TIME': [],        # Temps
            'MONEY': [],       # Montants
            'MISC': []         # Divers
        }
        
        for ent in doc.ents:
            entity_info = {
                'text': ent.text,
                'start': ent.start_char,
                'end': ent.end_char,
                'label_': ent.label_
            }
            
            if ent.label_ in entities_by_type:
                entities_by_type[ent.label_].append(entity_info)
            else:
                entities_by_type['MISC'].append(entity_info)
        
        # Supprimer les catÃ©gories vides
        return {k: v for k, v in entities_by_type.items() if v}
    
    def analyze_dependencies(self, text: str) -> Dict[str, Any]:
        print(f"[NLPProcessor] Analyse des dÃ©pendances pour: '{text}'")
        """
        Analyser la structure syntaxique.
        
        Args:
            text (str): Le texte Ã  analyser
            
        Returns:
            dict: Dictionnaire contenant l'analyse des dÃ©pendances
        """
        doc = self.nlp(text)
        
        dependencies = []
        for token in doc:
            dep_info = {
                'text': token.text,
                'pos': token.pos_,
                'dep': token.dep_,
                'head': token.head.text,
                'children': [child.text for child in token.children],
                'is_root': token.dep_ == 'ROOT'
            }
            dependencies.append(dep_info)
        
        # Identifier les racines (verbes principaux)
        roots = [token.text for token in doc if token.dep_ == 'ROOT']
        
        # Phrases
        sentences = []
        for sent in doc.sents:
            sent_info = {
                'text': sent.text,
                'root': sent.root.text,
                'start': sent.start_char,
                'end': sent.end_char
            }
            sentences.append(sent_info)
        
        return {
            'dependencies': dependencies,
            'roots': roots,
            'sentences': sentences,
            'tree_structure': self._build_dependency_tree(doc)
        }
    
    def _build_dependency_tree(self, doc) -> List[Dict]:
        """
        Construit un arbre des dÃ©pendances simplifiÃ©.
        
        Args:
            doc: Document spaCy
            
        Returns:
            list: Structure arborescente des dÃ©pendances
        """
        tree = []
        for token in doc:
            if token.dep_ == 'ROOT':
                tree.append(self._get_subtree(token))
        return tree
    
    def _get_subtree(self, token) -> Dict:
        """
        RÃ©cupÃ¨re le sous-arbre d'un token.
        
        Args:
            token: Token spaCy
            
        Returns:
            dict: Sous-arbre du token
        """
        return {
            'text': token.text,
            'pos': token.pos_,
            'dep': token.dep_,
            'children': [self._get_subtree(child) for child in token.children]
        }
    
    def full_analysis(self, text: str, verbose: bool = True) -> Dict[str, Any]:
        print(f"[NLPProcessor] Analyse complÃ¨te pour: '{text}'")
        """
        Analyse complÃ¨te d'un texte.
        
        Args:
            text (str): Le texte Ã  analyser
            verbose (bool): Afficher les dÃ©tails
            
        Returns:
            dict: Analyse complÃ¨te
        """
        if verbose:
            print(f"\nğŸ“ Analyse du texte: '{text}'")
            print("=" * 60)
        
        # Analyses
        pos_result = self.analyze_pos(text)
        entities_result = self.extract_entities(text)
        dependencies_result = self.analyze_dependencies(text)
        
        if verbose:
            self._print_analysis_results(pos_result, entities_result, dependencies_result)
        
        return {
            'original_text': text,
            'pos_analysis': pos_result,
            'entities': entities_result,
            'dependencies': dependencies_result
        }
    
    def _print_analysis_results(self, pos_result, entities_result, dependencies_result):
        """Affiche les rÃ©sultats d'analyse de maniÃ¨re formatÃ©e."""
        
        print("\nğŸ”¤ ANALYSE MORPHO-SYNTAXIQUE:")
        print("-" * 40)
        for token in pos_result['pos_analysis']:
            if token['is_alpha']:  # Afficher seulement les mots
                print(f"  {token['text']:15} | {token['pos']:8} | {token['lemma']:15} | {token['dep']:10}")
        
        print("\nğŸ‘¤ ENTITÃ‰S NOMMÃ‰ES:")
        print("-" * 40)
        if entities_result:
            for entity_type, entities in entities_result.items():
                print(f"  {entity_type}:")
                for entity in entities:
                    print(f"    - {entity['text']} ({entity['label_']})")
        else:
            print("  Aucune entitÃ© dÃ©tectÃ©e")
        
        print("\nğŸŒ³ DÃ‰PENDANCES SYNTAXIQUES:")
        print("-" * 40)
        for sent in dependencies_result['sentences']:
            print(f"  Phrase: {sent['text']}")
            print(f"  Racine: {sent['root']}")
        
        print("\nğŸ”— RELATIONS:")
        print("-" * 40)
        for dep in dependencies_result['dependencies']:
            if dep['is_root']:
                print(f"  ROOT: {dep['text']} ({dep['pos']})")
            elif dep['dep'] in ['nsubj', 'dobj', 'iobj']:  # Relations importantes
                print(f"  {dep['dep']}: {dep['text']} â† {dep['head']}")


# Tests et exemples d'utilisation
if __name__ == "__main__":
    print("ğŸ§  Module NLP Processor - Analyse linguistique avec spaCy")
    print("=" * 70)
    
    # Initialiser le processeur
    try:
        processor = NLPProcessor()
    except OSError:
        print("âŒ Impossible de charger le modÃ¨le spaCy")
        exit(1)
    
    # 5 phrases de test diffÃ©rentes
    test_sentences = [
        "Bonjour ! J'aimerais commander une pizza margherita s'il vous plaÃ®t.",
        "Pouvez-vous me dire les horaires d'ouverture de votre restaurant ?",
        "Je voudrais annuler ma commande prÃ©cÃ©dente, merci beaucoup.",
        "Marie Dupont travaille chez Google Ã  Paris depuis janvier 2023.",
        "Le prÃ©sident Emmanuel Macron s'est rendu Ã  Berlin hier pour rencontrer Angela Merkel."
    ]
    
    print(f"\nğŸ“‹ Tests sur {len(test_sentences)} phrases diffÃ©rentes:")
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ§ª TEST {i}")
        print(f"{'='*70}")
        
        result = processor.full_analysis(sentence, verbose=True)
        
        print(f"\nâœ… Test {i} terminÃ©")
    
    print(f"\nğŸ‰ Tous les tests terminÃ©s!")
    print("\nğŸ’¡ Utilisation:")
    print("```python")
    print("processor = NLPProcessor()")
    print("result = processor.full_analysis('Votre texte ici')")
    print("entities = processor.extract_entities('Votre texte')")
    print("dependencies = processor.analyze_dependencies('Votre texte')")
    print("```")
